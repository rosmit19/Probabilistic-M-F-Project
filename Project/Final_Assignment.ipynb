{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209b58ca-4c1d-4daf-8c4c-6824492ed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cf8af3-9f7f-4761-a40c-116c55c103c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a download option:\n",
      "1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)\n",
      "2) Kuzushiji-49 (49 classes, 28x28, 270k examples)\n",
      "3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a download option:\n",
      "1) NumPy data format (.npz)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading k49-train-imgs.npz - 64.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 64569/64569 [19:36<00:00, 54.88KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading k49-train-labels.npz - 0.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 219.29KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading k49-test-imgs.npz - 10.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10715/10715 [00:10<00:00, 1019.10KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading k49-test-labels.npz - 0.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 92.77KB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataset files downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
    "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
    "\n",
    "# Python2 compatibility\n",
    "try:\n",
    "    input = raw_input\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "download_dict = {\n",
    "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
    "        '1) MNIST data format (ubyte.gz)':\n",
    "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
    "        '2) NumPy data format (.npz)':\n",
    "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
    "    },\n",
    "    '2) Kuzushiji-49 (49 classes, 28x28, 270k examples)': {\n",
    "        '1) NumPy data format (.npz)':\n",
    "            ['http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz',\n",
    "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz'],\n",
    "    },\n",
    "    '3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)': {\n",
    "        '1) Folders of images (.tar)':\n",
    "            ['http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar'],\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Download a list of files\n",
    "def download_list(url_list):\n",
    "    for url in url_list:\n",
    "        path = url.split('/')[-1]\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            total_length = int(r.headers.get('content-length'))\n",
    "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
    "\n",
    "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print('All dataset files downloaded!')\n",
    "\n",
    "# Ask the user about which path to take down the dict\n",
    "def traverse_dict(d):\n",
    "    print('Please select a download option:')\n",
    "    keys = sorted(d.keys())  # Print download options\n",
    "    for key in keys:\n",
    "        print(key)\n",
    "\n",
    "    userinput = input('> ').strip()\n",
    "\n",
    "    try:\n",
    "        selection = int(userinput) - 1\n",
    "    except ValueError:\n",
    "        print('Your selection was not valid')\n",
    "        traverse_dict(d)  # Try again if input was not valid\n",
    "        return\n",
    "\n",
    "    selected = keys[selection]\n",
    "\n",
    "    next_level = d[selected]\n",
    "    if isinstance(next_level, list):  # If we've hit a list of downloads, download that list\n",
    "        download_list(next_level)\n",
    "    else:\n",
    "        traverse_dict(next_level)     # Otherwise, repeat with the next level\n",
    "\n",
    "traverse_dict(download_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76220df-02b8-4361-8218-2a3cb97e92c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31409513-eeff-4b79-811f-fae8fc23f2d7",
   "metadata": {},
   "source": [
    "Step1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0023653-65c0-4204-b286-36bdefb7f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867e3639-be29-48b0-bc9a-a8d6403b4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images shape: (38547, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "test_images = np.load(\"C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-test-imgs.npz\")['arr_0']\n",
    "print(f\"Test images shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7587b9a-b604-447c-bdc3-67d522106262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels shape: (38547,)\n"
     ]
    }
   ],
   "source": [
    "test_labels = np.load(\"C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-test-labels.npz\")['arr_0']\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d42ec728-13c6-46c5-a523-a59ba3da7377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (232365, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.load(\"C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-train-imgs.npz\")['arr_0']\n",
    "print(f\"Train images shape: {train_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d30039b-aa12-4573-af9a-7548821f3781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape: (232365,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.load(\"C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-train-labels.npz\")['arr_0']\n",
    "print(f\"Train labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e6dd-ef67-4188-aa8f-12375345360f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08cfba53-87f2-4953-b9de-e35b0e74f749",
   "metadata": {},
   "source": [
    "Step2: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f16b8a9e-41f5-4e7b-8f08-fdcdc771c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b906c-865a-46d5-ace0-ec461b35f8cb",
   "metadata": {},
   "source": [
    "Normalisation: Scaling pixel values to the range[0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c59f89b7-f558-482d-847f-921765efbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465bdf1-427d-4c16-b4c0-15ada61227fb",
   "metadata": {},
   "source": [
    "One-hot Encoding: Converting the labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f0bff2-e786-49f5-857d-85dafb15f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for one-hot encoding using PyTorch\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601776cd-31e2-4bc2-8eb9-b4bbd9e872aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to PyTorch tensors\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d9a3077-0f03-4c18-9fa8-e6b25d8b4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "num_classes = 49\n",
    "train_labels_onehot = to_one_hot(train_labels_tensor, num_classes)\n",
    "test_labels_onehot = to_one_hot(test_labels_tensor, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff65dbd-3495-4aa2-a00e-37cc62c6fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to PyTorch tensors and add a channel dimension\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7549bc90-a4dd-4467-b4e7-6378f8eff515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_onehot)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07a97480-9e03-40e1-be75-8ab6e9946d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_images_tensor, test_labels_onehot)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e4f1a27-b90a-4987-8058-921cf72b7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images tensor shape: torch.Size([232365, 1, 28, 28])\n",
      "Train labels tensor shape: torch.Size([232365, 49])\n",
      "Test images tensor shape: torch.Size([38547, 1, 28, 28])\n",
      "Test labels tensor shape: torch.Size([38547, 49])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train images tensor shape: {train_images_tensor.shape}\")\n",
    "print(f\"Train labels tensor shape: {train_labels_onehot.shape}\")\n",
    "print(f\"Test images tensor shape: {test_images_tensor.shape}\")\n",
    "print(f\"Test labels tensor shape: {test_labels_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e240e-90d9-4502-96de-76f175b7b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0af20a9f-7ff2-444a-808c-7f20b4899906",
   "metadata": {},
   "source": [
    "Step3: Implement the C-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b98b8e2a-7570-4a21-a2ac-e615ea60d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_classes):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim * 2)  # for mean and log variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        x = torch.cat((x, labels), dim=1)\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, labels):\n",
    "        z = torch.cat((z, labels), dim=1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, log_var = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z, labels), mu, log_var\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 28 * 28\n",
    "latent_dim = 20\n",
    "num_classes = 49\n",
    "\n",
    "cvae = CVAE(input_dim, latent_dim, num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cca6f88-340f-468a-a4c7-c92b8b041f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a656606-8b3e-4cf2-aaec-c7ea58eafa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3317353671155197\n",
      "Epoch 2, Loss: 0.03031778157602565\n",
      "Epoch 3, Loss: 0.0294548522530056\n",
      "Epoch 4, Loss: 0.029196554202162123\n",
      "Epoch 5, Loss: 0.029025710264441555\n",
      "Epoch 6, Loss: 0.028960623375977986\n",
      "Epoch 7, Loss: 0.028943588234857903\n",
      "Epoch 8, Loss: 0.028938769635464053\n",
      "Epoch 9, Loss: 0.028935935065162125\n",
      "Epoch 10, Loss: 0.028934330460362048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = cvae(data, labels)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bd573-abe9-45da-b020-b7fa9ff119ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1af35f-e54a-45da-8438-7adf02da6995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbc5ad-405b-4408-8aa7-59a279545cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e29577b3-5db2-4c78-b934-cf93a97eebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "train_images = np.load('C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-train-imgs.npz')['arr_0']\n",
    "train_labels = np.load('C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-train-labels.npz')['arr_0']\n",
    "test_images = np.load('C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-test-imgs.npz')['arr_0']\n",
    "test_labels = np.load('C:/Users/rosmi/OneDrive/Desktop/Probs Model n Inference/k49-test-labels.npz')['arr_0']\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9084000b-2da3-4081-a78f-47560bdfe133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function for one-hot encoding using PyTorch\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels]\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = 49\n",
    "train_labels_onehot = to_one_hot(train_labels_tensor, num_classes)\n",
    "test_labels_onehot = to_one_hot(test_labels_tensor, num_classes)\n",
    "\n",
    "# Convert images to PyTorch tensors and add a channel dimension\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86208d90-a6d4-4411-bb3d-e64dd397aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the C-VAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_size, label_size, hidden_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.label_size = label_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder_fc1 = nn.Linear(image_size + label_size, hidden_dim)\n",
    "        self.encoder_fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.encoder_fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + label_size, hidden_dim)\n",
    "        self.decoder_fc2 = nn.Linear(hidden_dim, image_size)\n",
    "    \n",
    "    def encode(self, x, c):\n",
    "        h = F.relu(self.encoder_fc1(torch.cat([x, c], dim=1)))\n",
    "        return self.encoder_fc2_mean(h), self.encoder_fc2_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        h = F.relu(self.decoder_fc1(torch.cat([z, c], dim=1)))\n",
    "        return torch.sigmoid(self.decoder_fc2(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        mean, logvar = self.encode(x.view(-1, self.image_size), c)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z, c), mean, logvar\n",
    "\n",
    "# Create an instance of the C-VAE model\n",
    "image_size = 28 * 28  # 28x28 images\n",
    "label_size = 49  # 49 classes\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "cvae = CVAE(image_size, label_size, hidden_dim, latent_dim)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d10f0b3b-f033-40ab-aae1-9fda93fcff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 221.85297518069325\n",
      "Epoch 2, Loss: 216.19826013070383\n",
      "Epoch 3, Loss: 216.59968723901832\n",
      "Epoch 4, Loss: 216.7093247339575\n",
      "Epoch 5, Loss: 216.58750019119043\n",
      "Epoch 6, Loss: 216.412949623287\n",
      "Epoch 7, Loss: 215.94104459332166\n",
      "Epoch 8, Loss: 215.76290262107494\n",
      "Epoch 9, Loss: 215.61836529382626\n",
      "Epoch 10, Loss: 215.55167309578388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_images_tensor)):\n",
    "        img = train_images_tensor[i].view(-1, 28*28)\n",
    "        lbl = train_labels_onehot[i].view(1, -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mean, logvar = cvae(img, lbl)\n",
    "        loss = loss_function(recon_batch, img, mean, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(train_images_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e46fe-788e-4d7b-92a9-636642c76ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to generate new images\n",
    "def generate_images(cvae, num_samples, label):\n",
    "    cvae.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        labels = torch.eye(num_classes)[torch.tensor([label] * num_samples)]\n",
    "        generated_images = cvae.decode(z, labels).view(-1, 1, 28, 28)\n",
    "    return generated_images\n",
    "\n",
    "# Generate and visualize new images\n",
    "def show_generated_images(images, num_images):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i, 0, :, :], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate 5 new images for a specific class (e.g., class 0)\n",
    "generated_images = generate_images(cvae, num_samples=5, label=0)\n",
    "show_generated_images(generated_images, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d352451-d61b-4937-8c80-8409746440b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635cae9-f737-4491-aa33-530fb6d5ef27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0725a532-71fb-49c0-9d64-ca4832518394",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms, datasets\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, label_dim, img_dim):\n",
    "        super(CGANGenerator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(label_dim, label_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim + label_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, momentum=0.8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, momentum=0.8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([noise, c], 1)\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dd896-3489-459b-b12d-88601816bf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
